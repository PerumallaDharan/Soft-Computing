{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Python program to implement a MADALINE on logical XOR function. Take the binary inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Predictions after training:\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MADALINE:\n",
    "    def __init__(self, learning_rate=0.1, iterations=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        \n",
    "        self.w = np.random.uniform(-0.5, 1, (2, 2)) \n",
    "        self.b1 = np.random.uniform(-0.5, 1, 2)      \n",
    "        \n",
    "\n",
    "        self.v = np.random.uniform(-0.5, 1, (2, 1))  \n",
    "        self.b2 = np.random.uniform(-0.5, 1, 1)      \n",
    "\n",
    "\n",
    "    def fx(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        z_in = np.dot(X, self.w) + self.b1  \n",
    "        z_out = self.fx(z_in)      \n",
    "        \n",
    "        y_in = np.dot(z_out, self.v) + self.b2  \n",
    "        y_out = self.fx(y_in)           \n",
    "        \n",
    "        return y_out\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.iterations):\n",
    "            for i in range(len(X)):\n",
    "\n",
    "                z_in = np.dot(X[i], self.w) + self.b1  \n",
    "                z_out = self.fx(z_in)         \n",
    "                \n",
    "                y_in = np.dot(z_out, self.v) + self.b2 \n",
    "                y_out = self.fx(y_in)         \n",
    "\n",
    "                if y_out != y[i]:\n",
    "                    if y[i] == -1:\n",
    "                        for j in range(len(z_in)):\n",
    "                            if z_in[j] > 0:\n",
    "\n",
    "                                self.w[:, j] += (self.learning_rate * (X[i]) *(y[i]-z_in[j].item())) \n",
    "                                self.b1[j] += (self.learning_rate * (y[i]-z_in[j].item()))       \n",
    "\n",
    "                    elif y[i] == 1:\n",
    "\n",
    "                        closest_to_zero_index = np.argmin(np.abs(z_in))\n",
    "                        \n",
    "                        self.w[:, closest_to_zero_index] += (self.learning_rate * X[i] *(y[i]-z_in[j].item()))  \n",
    "                        self.b1[closest_to_zero_index] += (self.learning_rate *(y[i]-z_in[closest_to_zero_index].item()))      \n",
    "\n",
    "# Input Data for training (XOR logic)\n",
    "X = np.array([[0,0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Input Data for testing (same as XOR)\n",
    "Z =np.array([[0,0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "madaline = MADALINE(learning_rate=0.1, iterations=10000)\n",
    "\n",
    "madaline.train(X, y)\n",
    "\n",
    "predictions = madaline.predict(Z)\n",
    "\n",
    "# Display results\n",
    "print(\"Testing Data:\")\n",
    "print(Z)\n",
    "print(\"Predictions after training:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100,\n",
      "\n",
      "Testing MADALINE on XOR function:\n",
      "Input: [0 0] -> Output: 1\n",
      "Input: [0 1] -> Output: 1\n",
      "Input: [1 0] -> Output: 1\n",
      "Input: [1 1] -> Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function\n",
    "def activation_function(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "# Initialize weights and bias\n",
    "w11, w21 = np.random.rand(), np.random.rand()\n",
    "w12, w22 = np.random.rand(), np.random.rand()\n",
    "v1, v2, b1, b2, b3 = 0.5, 0.5, 0.5, 0.5, 0.5\n",
    "alpha = 0.1  # Learning rate\n",
    "\n",
    "# XOR input and output\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "T = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        x1, x2 = X[i]\n",
    "\n",
    "        # Step 5: Compute net inputs of Adaline units\n",
    "        zin1 = b1 + x1 * w11 + x2 * w21\n",
    "        zin2 = b2 + x1 * w12 + x2 * w22\n",
    "\n",
    "        # Step 6: Output of Adaline units\n",
    "        z1 = activation_function(zin1)\n",
    "        z2 = activation_function(zin2)\n",
    "\n",
    "        # Step 7: Net input to output unit\n",
    "        yin = b3 + z1 * v1 + z2 * v2\n",
    "\n",
    "        # Apply activation function to get the output\n",
    "        y = activation_function(yin)\n",
    "\n",
    "        # Step 8: Calculate the error and update weights if needed\n",
    "        t = T[i]\n",
    "        if t != y:\n",
    "            if t == 1:\n",
    "                # Update weights for z unit with net input closest to 0\n",
    "                if abs(zin1) < abs(zin2):\n",
    "                    w11 += alpha * (t - zin1) * x1\n",
    "                    w21 += alpha * (t - zin1) * x2\n",
    "                    b1 += alpha * (t - zin1)\n",
    "                else:\n",
    "                    w12 += alpha * (t - zin2) * x1\n",
    "                    w22 += alpha * (t - zin2) * x2\n",
    "                    b2 += alpha * (t - zin2)\n",
    "            else:\n",
    "                # Update weights for z units with positive net input\n",
    "                if zin1 > 0:\n",
    "                    w11 += alpha * (t - zin1) * x1\n",
    "                    w21 += alpha * (t - zin1) * x2\n",
    "                    b1 += alpha * (t - zin1)\n",
    "                if zin2 > 0:\n",
    "                    w12 += alpha * (t - zin2) * x1\n",
    "                    w22 += alpha * (t - zin2) * x2\n",
    "                    b2 += alpha * (t - zin2)\n",
    "\n",
    "    # Stopping condition: Here, we just run for a fixed number of epochs\n",
    "    if epoch == epochs - 1:\n",
    "        print(f'Epochs : {epoch + 1},')\n",
    "# Testing the trained MADALINE on the XOR function\n",
    "print(\"\\nTesting MADALINE on XOR function:\")\n",
    "for i in range(len(X)):\n",
    "    x1, x2 = X[i]\n",
    "\n",
    "    # Compute net inputs of Adaline units\n",
    "    zin1 = b1 + x1 * w11 + x2 * w21\n",
    "    zin2 = b2 + x1 * w12 + x2 * w22\n",
    "\n",
    "    # Output of Adaline units\n",
    "    z1 = activation_function(zin1)\n",
    "    z2 = activation_function(zin2)\n",
    "\n",
    "    # Net input to output unit\n",
    "    yin = b3 + z1 * v1 + z2 * v2\n",
    "\n",
    "    # Apply activation function to get the output\n",
    "    y = activation_function(yin)\n",
    "    print(f'Input: {X[i]} -> Output: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
