{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8578310-c705-410b-9aab-7aba42018bed",
   "metadata": {},
   "source": [
    "### Write a Python program to train a Back Propagation Neural Network (BPNN) for classifying whether a student passes or fails using a dataset of students' course marks. Assume necessary parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e041f06f-267d-4d62-a330-de80a9f950c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\opdha\\AppData\\Local\\Temp\\ipykernel_14144\\2600632885.py:26: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_path = r'E:\\SRM\\Soft Computing\\Lab 7 - 25th Sept\\training_dataset_students(1000).csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_path = r'E:\\SRM\\Soft Computing\\Lab 7 - 25th Sept\\students_testing.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "X_train = train_df[['c1', 'c2', 'c3', 'c4', 'c5', 'c6']].values\n",
    "y_train = train_df[['result']].values\n",
    "\n",
    "X_test = test_df[['c1', 'c2', 'c3', 'c4', 'c5', 'c6']].values\n",
    "y_test = test_df[['result']].values\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.01, iterations=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights_input_hidden = np.random.uniform(-0.5, 0.5, (input_size, hidden_size))\n",
    "        self.bias_hidden = np.random.uniform(-0.5, 0.5, (hidden_size,))\n",
    "        self.weights_hidden_output = np.random.uniform(-0.5, 0.5, (hidden_size, 1))\n",
    "        self.bias_output = np.random.uniform(-0.5, 0.5, (1,))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        final_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        final_output = self.sigmoid(final_input)\n",
    "        return np.round(final_output)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.iterations):\n",
    "            for i in range(len(X)):\n",
    "                # Forward pass\n",
    "                hidden_input = np.dot(X[i], self.weights_input_hidden) + self.bias_hidden\n",
    "                hidden_output = self.sigmoid(hidden_input)\n",
    "                final_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "                final_output = self.sigmoid(final_input)\n",
    "\n",
    "                #Finding gradiants and errors\n",
    "                output_error = (y[i] - final_output)  \n",
    "                output_gradient = output_error * self.sigmoid_derivative(final_output) \n",
    "\n",
    "                hidden_error = output_gradient.dot(self.weights_hidden_output.T)  \n",
    "                hidden_gradient = hidden_error * self.sigmoid_derivative(hidden_output)  \n",
    "\n",
    "                # Update weights and biases\n",
    "                #updating weights (V) and bias\n",
    "                self.weights_hidden_output += self.learning_rate * hidden_output[:, None] * output_gradient  \n",
    "                self.bias_output += self.learning_rate * output_gradient  \n",
    "                \n",
    "                #updating weights (W) and bias\n",
    "                self.weights_input_hidden += self.learning_rate * X[i][:, None] * hidden_gradient  \n",
    "                self.bias_hidden += self.learning_rate \n",
    "\n",
    "\n",
    "# Model configuration\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 6\n",
    "\n",
    "mlp = MLP(input_size=input_size, hidden_size=hidden_size, learning_rate=0.01, iterations=1000)\n",
    "mlp.train(X_train, y_train)\n",
    "\n",
    "# Test predictions and accuracy\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24414208-fc51-4831-99ed-0f1dd74cd0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
